{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b127ef",
   "metadata": {},
   "source": [
    "<h1>Resumen de Implementación</h1>\n",
    "<h2>Objetivo</h2>\n",
    "    <p>El código entrena un modelo de deep learning para detectar puntos característicos en rostros.</p>\n",
    "\n",
    "<h2>Descripción</h2>\n",
    "    <ul>\n",
    "        <li>Implementa un modelo de regresión basado en ResNet-18 para detectar puntos clave en rostros.</li>\n",
    "        <li>El modelo estima 136 valores (68 puntos, cada uno con coordenadas x e y).</li>\n",
    "    </ul>\n",
    "\n",
    "<h2>Implementación Base</h2>\n",
    "    <ul>\n",
    "        <li>El código sigue la implementación de ResNet-18.</li>\n",
    "        <li>Los datos de entrenamiento y validación se cargan desde archivos CSV.</li>\n",
    "        <li>Se utiliza MSE como función de pérdida.</li>\n",
    "    </ul>\n",
    "\n",
    "<h2>Datos de Entrenamiento y Validación</h2>\n",
    "    <p>Se asume que los datos de entrenamiento y validación están en archivos CSV proporcionados.</p>\n",
    "\n",
    "   <h2>Función de Pérdida</h2>\n",
    "   <p>Se utiliza MSE (Mean Square Error).</p>\n",
    "\n",
    "<h2>Métricas</h2>\n",
    "    <ul>\n",
    "        <li>El código evalúa el modelo usando RMSE (Root Mean Square Error).</li>\n",
    "        <li>También muestra visualmente las predicciones comparando los puntos reales y predichos.</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from resnet import ResNetBackbone\n",
    "from retinaface import RetinaFace  # Asegúrate de instalar retinaface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efedb101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2: Definir el dataset de puntos característicos de rostros\n",
    "class FaceKeypointsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.keypoints_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keypoints_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.keypoints_frame.iloc[idx, 0])\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        keypoints = self.keypoints_frame.iloc[idx, 1:].values\n",
    "        keypoints = keypoints.astype('float').reshape(-1, 2)\n",
    "\n",
    "        # Detección de rostros usando RetinaFace\n",
    "        faces = RetinaFace.detect_faces(image)\n",
    "        if faces:\n",
    "            face = faces[list(faces.keys())[0]]  # Usar la primera cara detectada\n",
    "            facial_area = face['facial_area']\n",
    "            image = image[facial_area[1]:facial_area[3], facial_area[0]:facial_area[2]]\n",
    "\n",
    "        sample = {'image': image, 'keypoints': keypoints}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140976b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2: Definir las transformaciones\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb036182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2.2: Cargar los datasets\n",
    "train_dataset = FaceKeypointsDataset(csv_file='FaceKPoints/training_frames_keypoints.csv', root_dir='FaceKPoints/training', transform=data_transform)\n",
    "valid_dataset = FaceKeypointsDataset(csv_file='FaceKPoints/valid_frames_keypoints.csv', root_dir='FaceKPoints/valid', transform=data_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b056ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2: Definir el modelo basado en ResNet de resnet.py\n",
    "class KeypointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeypointModel, self).__init__()\n",
    "        # Usar el backbone de ResNet desde resnet.py\n",
    "        self.resnet_backbone = ResNetBackbone(block_sizes=[2, 2, 2, 2], filters=[64, 128, 256, 512])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 136)  # 68 puntos clave * 2 (x, y)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet_backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2.3: Inicializar el modelo, la función de pérdida y el optimizador\n",
    "#model = KeypointModel()\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aeb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4eed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2.3: Compilar y entrenar el modelo\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f'Época {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            inputs = batch['image'].to(device)\n",
    "            keypoints = batch['keypoints'].view(batch['keypoints'].size(0), -1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, keypoints)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # imprimir cada 10 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = batch['image'].to(device)\n",
    "                keypoints = batch['keypoints'].view(batch['keypoints'].size(0), -1).to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, keypoints)\n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {valid_loss / len(valid_loader):.3f}')\n",
    "        model.train()\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el proceso de entrenamiento\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 5: Guardar el modelo\n",
    "torch.save(model.state_dict(), 'model-res.pth')\n",
    "print('Model saved as model-res.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2.4: Evaluación del modelo usando RMSE\n",
    "def evaluate_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    total_rmse = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            images = batch['image']\n",
    "            keypoints = batch['keypoints'].view(batch['keypoints'].size(0), -1)\n",
    "            outputs = model(images)\n",
    "            mse = criterion(outputs, keypoints)\n",
    "            rmse = torch.sqrt(mse)\n",
    "            total_rmse += rmse.item() * images.size(0)\n",
    "            n_samples += images.size(0)\n",
    "\n",
    "    avg_rmse = total_rmse / n_samples\n",
    "    print(f'RMSE en el conjunto de validación: {avg_rmse:.4f}')\n",
    "    return avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabaa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "rmse = evaluate_model(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b983ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2.4: Mostrar algunos resultados cualitativos\n",
    "def visualize_predictions(model, valid_loader, num_images=5):\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            images = batch['image']\n",
    "            keypoints = batch['keypoints'].view(batch['keypoints'].size(0), -1)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                if images_so_far >= num_images:\n",
    "                    return\n",
    "                image = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "                true_keypoints = keypoints[i].view(-1, 2).cpu().numpy()\n",
    "                predicted_keypoints = outputs[i].view(-1, 2).cpu().numpy()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(image)\n",
    "                plt.scatter(true_keypoints[:, 0], true_keypoints[:, 1], c='r', marker='o')\n",
    "                plt.scatter(predicted_keypoints[:, 0], predicted_keypoints[:, 1], c='b', marker='x')\n",
    "                plt.title('Puntos Reales (rojo) vs Predichos (azul)')\n",
    "                plt.show()\n",
    "\n",
    "                images_so_far += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar las predicciones\n",
    "visualize_predictions(model, valid_loader, num_images=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
